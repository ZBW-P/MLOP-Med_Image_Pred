name: xray-etl

volumes:
  xraydata:

services:
  extract-data:
    container_name: etl_xray_extract
    image: python:3.11
    user: root
    volumes:
      - xraydata:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e

        echo "Resetting dataset directory..."
        rm -rf xray_dataset
        mkdir -p xray_dataset
        cd xray_dataset

        echo "Downloading X-ray dataset zip..."
        curl -L -o xraynumpy.zip https://www.kaggle.com/api/v1/datasets/download/bibhash123/xraynumpy

        echo "Unzipping X-ray dataset..."
        unzip -q xraynumpy.zip
        rm -f xraynumpy.zip

        echo "Downloading CSV metadata..."
        curl -L -o train.csv.zip "https://storage.googleapis.com/kaggle-data-sets/1042002/1794153/compressed/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1746941105&Signature=kgpzZ9QZD5dD5gG5gbEqCznf%2B6qK%2Bh5lPx59s0BVQ1qjNUZ0QJlMbQKkwBvJAiYmz6HFA8apXjqD2dxz1cTJCTmQrMpGgpco8caxE86Wnamh0Mp5emZEc5eFYv3oYUHFqjJHGywTquOQfAFu3UPJOTjA%2BlG47pq0eafR8FoDmTbFcPcN841pMKyrUtXwCPQ8NnwM7C6bNv6oLnNHsHa8KCu4ov9asF7CseRRiSl%2FKUDxAE2grpYjanb%2BS6bxA8ZJOA0%2FA2c6JZ%2BGEzPEc4t4ScI3xxiDxZhleYw%2FJXi4FIOBkTRA39yOSxeU%2Bl9JuM%2F0ue0U6eoqJrQ0tQRorUkDEg%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv.zip"

        echo "Unzipping CSV metadata..."
        unzip -q train.csv.zip
        rm -f train.csv.zip

        echo "Listing contents of /data after extract stage:"
        ls -l /data/xray_dataset

  transform-data:
    container_name: etl_xray_transform
    image: python:3.11
    volumes:
      - xraydata:/data
    working_dir: /data/xray_dataset
    command:
      - bash
      - -c
      - |
        set -e

        echo "Installing Python dependencies..."
        pip install pandas pillow numpy scikit-learn

        echo "Starting data transformation and split..."
        python3 - <<EOF
import os
import pandas as pd
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split

# Define paths
train_dir = "archive/images/train"
output_base_dir = "/data/final_datasets"
csv_path = "train.csv"

# Create output directory if it does not exist
os.makedirs(output_base_dir, exist_ok=True)

# Load CSV metadata
df = pd.read_csv(csv_path)

# Build a dictionary mapping image_id to class_name
id_to_class = dict(zip(df['image_id'], df['class_name']))

# Build a list of all samples as (npy_path, class_name)
samples = []
for filename in os.listdir(train_dir):
    if filename.endswith('.npy'):
        image_id = filename.replace('.npy', '')
        npy_path = os.path.join(train_dir, filename)
        class_name = id_to_class.get(image_id, 'Unknown')
        samples.append((npy_path, class_name))

# Create DataFrame for easy manipulation
df_samples = pd.DataFrame(samples, columns=['path', 'class_name'])

# Perform stratified split: 70% train, 30% temp
train_df, temp_df = train_test_split(
    df_samples,
    test_size=0.3,
    stratify=df_samples['class_name'],
    random_state=42
)

# Split temp into 10% val, 20% test (to maintain 7:1:2 ratio)
val_df, test_df = train_test_split(
    temp_df,
    test_size=2/3,
    stratify=temp_df['class_name'],
    random_state=42
)

# Prepare dictionary for splits
dataset_splits = {
    'train': train_df,
    'val': val_df,
    'test': test_df
}

# Process and save each image
for split_name, split_df in dataset_splits.items():
    print(f"Processing {split_name} set with {len(split_df)} samples...")
    for _, row in split_df.iterrows():
        npy_path = row['path']
        class_name = row['class_name']
        image_id = os.path.basename(npy_path).replace('.npy', '')

        # Load the numpy array and convert to PIL Image
        img_array = np.load(npy_path)
        img = Image.fromarray(img_array).convert('L')

        # Build output path
        class_dir = os.path.join(output_base_dir, split_name, class_name)
        os.makedirs(class_dir, exist_ok=True)
        png_path = os.path.join(class_dir, f"{image_id}.png")

        # Save as PNG
        img.save(png_path)

print("Data transformation and split completed successfully.")
EOF

        echo "Listing contents of /data/final_datasets after transform stage:"
        find /data/final_datasets -type f | head -n 10

  load-data:
    container_name: etl_xray_load
    image: rclone/rclone:latest
    volumes:
      - xraydata:/data
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ -z "$RCLONE_CONTAINER" ]; then
          echo "ERROR: RCLONE_CONTAINER is not set"
          exit 1
        fi
        echo "Cleaning up existing contents in the remote container..."
        rclone delete chi_tacc:$RCLONE_CONTAINER --rmdirs || true

        echo "Uploading dataset to remote container..."
        rclone copy /data chi_tacc:$RCLONE_CONTAINER \
        --progress \
        --transfers=32 \
        --checkers=16 \
        --multi-thread-streams=4 \
        --fast-list

        echo "Listing directories in the remote container after upload:"
        rclone lsd chi_tacc:$RCLONE_CONTAINER
